{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "random.seed(102)\n",
    "\n",
    "from datasketching.minhash import SimpleMinhash\n",
    "from datasketching.minhash import murmurmaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/music.parquet\")\n",
    "\n",
    "df = df.drop(df[df[\"2\"].str.len() > 60].index) # we remove long band names.\n",
    "\n",
    "df.sample(10, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_minhash_sig(user_dat, nhash):\n",
    "    mh = SimpleMinhash(nhash)\n",
    "    for row in user_dat:\n",
    "        mh.add(row)\n",
    "    return mh\n",
    "\n",
    "def unique_artists(df):\n",
    "    return df['2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby(['0'])\n",
    "un_artists = grouped_df.apply(unique_artists)\n",
    "mh_sigs = un_artists.apply(generate_minhash_sig, nhash = 128)\n",
    "\n",
    "users = df['0'].unique()\n",
    "dusers = {x+1:y for x,y in enumerate(sorted(set(users)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_users = pd.DataFrame( columns=['user', 'artist','plays'])    \n",
    "ii = 0 \n",
    "\n",
    "for u in range(0, 992):    \n",
    "    print(u)\n",
    "    x = mh_sigs[u]\n",
    "    artists_listened = len(un_artists[u])\n",
    "    to_sample = int(np.floor(artists_listened)*0.05)\n",
    "    sim=[]\n",
    "    for mh in range(0, 992):\n",
    "        sim.append(mh_sigs[mh].similarity(mh_sigs[0]))\n",
    "    \n",
    "    similar = set(sorted(sim, reverse=True)[1:11]) # the ten largest similarities\n",
    "    similar_users = ([i for i, e in enumerate(sim) if e in similar]) # extract the user values\n",
    "    \n",
    "    \n",
    "    user_play_fr = grouped_df.get_group(dusers[(u+1)]).groupby(['2']).count()['1'].values\n",
    "    \n",
    "    \n",
    "    for j in range(0, 50):\n",
    "        # print(j)\n",
    "        ### make 5 new users for each user\n",
    "        username = 'user' + f\"{u:03}\" + f\"{j:02}\"\n",
    "        #print(username)\n",
    "        selected = random.sample(similar_users, 6)\n",
    "        listened = []\n",
    "        for k in selected:\n",
    "            possible = np.setdiff1d(un_artists[k], (list(un_artists[u])+listened))\n",
    "            listened = listened + list(np.random.choice(un_artists[k], size = to_sample, replace = False))\n",
    "            \n",
    "        listened = listened + list(np.random.choice(un_artists[u], size=int(np.floor(artists_listened*0.7))))\n",
    "        \n",
    "        ### now simulate user plays. \n",
    "        user_plays = np.random.choice(user_play_fr, size=len(listened), replace = False)\n",
    "        \n",
    "        user_data = {'user':np.repeat(username,len(listened), axis=0) , 'artist':listened, 'plays':user_plays} \n",
    "        user_df = pd.DataFrame(user_data) \n",
    "        new_users = pd.concat([new_users, user_df])\n",
    "        \n",
    "    ii = ii + 1\n",
    "    #print(ii)\n",
    "    if ii == 20:\n",
    "        ### write file to parquet every 10th user, and begin a new file\n",
    "        filename='data/userdat'+str(u)+'.parquet'\n",
    "        print(filename)\n",
    "        new_users.to_parquet(filename)\n",
    "        ii = 0\n",
    "        new_users = pd.DataFrame( columns=['user', 'artist','plays'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='data/userdat'+str(u)+'.parquet'\n",
    "print(filename)\n",
    "new_users.to_parquet(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading data\n",
    " write it to parquet files grouped by artist - smaller than grouped by user. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_data = pd.DataFrame(columns=['user', 'artist','plays'])\n",
    "for j in f:\n",
    "    df = pd.read_parquet('data/'+j)\n",
    "    pseudo_data = pd.concat([pseudo_data, df])\n",
    "    print(pseudo_data.shape)\n",
    "    \n",
    "df = pseudo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove data that causes error \n",
    "df = df.drop(df[df['artist'].str.contains('G\\ufff6L￤')].index)\n",
    "df = df.drop(df[df['artist'].str.contains('ﾔﾵﾁﾸ\\uffd0\\uffd0\\uffd0\\uffd0\\uffd0')].index)\n",
    "df = df.drop(df[df[\"artist\"].str.contains('䐀攀愀琀栀\\u2000昀爀漀洀\\u2000䄀戀漀瘀攀\\u2000\\u3100㤀㜀㤀')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "table = pyarrow.Table.from_pandas(df)\n",
    "\n",
    "pq.write_to_dataset(\n",
    "    table, \n",
    "    root_path='partitioned_output.parquet',\n",
    "    partition_cols=['artist'],)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
