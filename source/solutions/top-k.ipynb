{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-K elements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasketching.cms as cms\n",
    "import heapq\n",
    "\n",
    "MAX_OVERHEAD = 10\n",
    "\n",
    "class TopK(object):\n",
    "\n",
    "    def __init__(self, k, width, hashes):\n",
    "        self.width = width\n",
    "        self.hashes = hashes\n",
    "        self.cms = cms.CMS(width, hashes)\n",
    "        self.k = k\n",
    "        self.queue = []\n",
    "        self.seen = set()\n",
    "\n",
    "    def insert(self, obj):\n",
    "        \"\"\" Inserts _obj_ in this summary and updates the top k elements if this\n",
    "            element is likely to be in the top k elements as given by the underlying top-k sketch \"\"\"\n",
    "\n",
    "        # Identify how many times you've seen `obj` already\n",
    "        self.cms.insert(obj)\n",
    "        count = self.cms.lookup(obj)\n",
    "\n",
    "        # Insert _obj_ into the priority queue (`self.queue`).\n",
    "\n",
    "        # Hint #1:  How would you sort the priority queue to ensure that it contains the top k elements?\n",
    "        # Hint #2:  Python's `heapq.heappush` function puts the smallest things first\n",
    "        # Hint #3:  What happens when you need to update the count of something that's already in the queue?\n",
    "\n",
    "        # By storing a tuple with the negated count as the first element, we can keep the smallest things first\n",
    "        to_insert = (-count, obj)\n",
    "\n",
    "        if obj in self.seen:\n",
    "            item, = [x for x in self.queue if x[1] == obj]\n",
    "            self.queue.remove(item)\n",
    "        else:\n",
    "            self.seen.add(obj)\n",
    "\n",
    "        heapq.heappush(self.queue, to_insert)\n",
    "\n",
    "        # Hint #4:  How will you ensure that the priority queue doesn't grow unbounded?\n",
    "        if len(self.queue) > (self.k * MAX_OVERHEAD):\n",
    "            newsize = max(int(self.k * MAX_OVERHEAD / 2), 1)\n",
    "            self.queue = heapq.nsmallest(newsize, self.queue)\n",
    "            self.seen = set([v for _, v in self.queue])\n",
    "\n",
    "\n",
    "    def topk(self):\n",
    "        \"\"\" Returns a list of 2-tuples (value, count) for the top k elements in this structure \"\"\"\n",
    "        return [(value, -count) for count, value in heapq.nsmallest(self.k, self.queue)]\n",
    "\n",
    "\n",
    "    def merge(self, other):\n",
    "        result = TopK(self.width, self.hashes)\n",
    "        result.cms.merge_from(self.cms)\n",
    "        result.cms.merge_from(other.cms)\n",
    "        newsize = max(int(self.k * MAX_OVERHEAD / 2), 1)\n",
    "        result.queue = heapq.nsmallest(newsize, heaqp.merge(self.queue, other.queue))\n",
    "        result.seen = set([v for _, v in result.queue])\n",
    "        result.k = self.k\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_experiment(sample_count, size, hashes, k=10, seed=0x15300625):\n",
    "    import random\n",
    "    from collections import namedtuple\n",
    "   \n",
    "    random.seed(seed)\n",
    "    topk = TopK(k, size, hashes)\n",
    "    \n",
    "    result = []\n",
    "    total_count = 0\n",
    "    \n",
    "    # update the counts\n",
    "    for i in range(sample_count):\n",
    "        bits = random.getrandbits(64)\n",
    "        if i % 100 == 0:\n",
    "            # every hundredth entry is a heavy hitter\n",
    "            insert_count = (bits % 512) + 1\n",
    "        else:\n",
    "            insert_count = (bits % 8) + 1\n",
    "        \n",
    "        for i in range(insert_count):\n",
    "            topk.insert(bits)\n",
    "    \n",
    "    return topk.topk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14364086215630792700, 517),\n",
       " (2991287392996463102, 516),\n",
       " (9676343778858885118, 511),\n",
       " (5115259641344439293, 510),\n",
       " (14529685338139490810, 507),\n",
       " (766813825201687544, 505),\n",
       " (4701884162691338739, 505),\n",
       " (5631968365117192696, 505),\n",
       " (14240245373571445239, 504),\n",
       " (2362392399177886197, 502),\n",
       " (7310823104820324852, 501),\n",
       " (4853657629704797682, 499),\n",
       " (7209717596912719346, 499),\n",
       " (5367578118589810665, 496),\n",
       " (5853828888560029671, 496),\n",
       " (2743391102434662886, 492),\n",
       " (17900334999078540778, 491),\n",
       " (5414524836017999845, 486),\n",
       " (486034635062679522, 483),\n",
       " (15682754308008826338, 483)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasketching.hashing import hashes_for\n",
    "topk_experiment(40000, 16384, hashes_for(3,8), k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
